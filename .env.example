# Ollama URL for the backend to connect
# The path '/ollama' will be redirected to the specified backend URL
OLLAMA_BASE_URL='http://localhost:11434'

OPENAI_API_BASE_URL=''
OPENAI_API_KEY=''

# OpenAI Model Configuration
# Default model to use (e.g., 'gpt-4.1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo')
OPENAI_MODEL='gpt-4.1-mini'

# AUTOMATIC1111_BASE_URL="http://localhost:7860"

# Snowflake Cortex Search Configuration
# Snowflake account identifier (e.g., 'account-name' or 'account-name.region')
SNOWFLAKE_ACCOUNT=''

# Snowflake user for authentication
SNOWFLAKE_USER=''

# Snowflake role to use
SNOWFLAKE_ROLE=''

# Snowflake warehouse to use
SNOWFLAKE_WAREHOUSE=''

# Snowflake database name
SNOWFLAKE_DATABASE=''

# Snowflake schema name
SNOWFLAKE_SCHEMA=''

# Snowflake Cortex Search service name
SNOWFLAKE_CORTEX_SERVICE=''

# Snowflake authentication - use either password or private key
# Option 1: Password authentication
# SNOWFLAKE_PASSWORD=''

# Option 2: Private key authentication (recommended for production)
# SNOWFLAKE_PRIVATE_KEY_PEM='-----BEGIN PRIVATE KEY-----\n...\n-----END PRIVATE KEY-----'

# RAG (Retrieval-Augmented Generation) Configuration
# Number of documents to retrieve from Snowflake Cortex Search (default: 4)
RAG_CONTEXT_LIMIT=4

# Enable user email-based filtering (requires Snowflake support)
ENABLE_USER_EMAIL_FILTER=false

# For production, you should only need one host as
# fastapi serves the svelte-kit built frontend and backend from the same host and port.
# To test with CORS locally, you can set something like
# CORS_ALLOW_ORIGIN='http://localhost:5173;http://localhost:8080'
CORS_ALLOW_ORIGIN='*'

# For production you should set this to match the proxy configuration (127.0.0.1)
FORWARDED_ALLOW_IPS='*'

# DO NOT TRACK
SCARF_NO_ANALYTICS=true
DO_NOT_TRACK=true
ANONYMIZED_TELEMETRY=false
